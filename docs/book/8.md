# Optimization Strategies for Edge Deployment

## Introduction

Optimization is the art of squeezing more performance from limited resources, ensuring models train and infer quickly on edge devices without sacrificing accuracy. This chapter focuses on strategies tailored for edge deployment: vectorization for CPUs, accelerated libraries (BLAS), GPU offloading via Metal (see `src/microgpt_metal.h`, `src/microgpt_metal.m`), quantization (INT8; see `QUANTIZATION_INT8` in `src/microgpt.h`), and memory management.

The key principle at MicroGPT-C's scale (under 1M parameters): simplicity often wins—fancy accelerations can backfire due to dispatch overhead, but smart low-level tweaks yield big gains.

## Vectorization: Leveraging CPU Parallelism

Modern CPUs can process multiple data points simultaneously through vector instructions, like SIMD (Single Instruction, Multiple Data).

Background for Beginners: Normally, a CPU adds numbers one by one. SIMD adds four (or more) at once, speeding matrix operations central to transformers (e.g., dot products in attention).

In MicroGPT-C, compiler flags like -O3 and -march=native enable auto-vectorization— the compiler rewrites loops to use SIMD without code changes.

Scenario: Matrix multiplication (matmul) in embeddings. 

**Visualization: Scalar vs. SIMD Execution**

![Scalar vs SIMD Execution](simd_bw.png)

Math Verification: For vectors of length n=128, scalar add requires $n$ operations. With a SIMD width of 4, it requires $n/4$ operations—approaching a 4x theoretical speedup. In reality, memory alignment overhead reduces this to ~3x, which is still massive for edge inference.

Example: Benchmark matmul 128x128. Scalar: 5ms; Vectorized: 1.5ms. On ARM architectures (common in mobile/IoT devices), NEON SIMD doubles or quadruples overall throughput.

Tradeoff: At tiny sizes (n<32), setup cost > benefit. Verify: Time small vs. large—crossover at n~64.

To ground this, here is an example of code designed for auto-vectorization. Note how keeping the inner loop simple and free of branching allows compilers like GCC/Clang to automatically emit SIMD instructions:
```c
// Benchmark Snippet: Matrix-Vector Product optimized for auto-vectorization
void matmul_forward(float* out, float* x, float* w, int n, int d) {
    // OpenMP parallelizes across threads, while -O3 auto-vectorizes the inner loop
    #pragma omp parallel for
    for (int i = 0; i < d; i++) {
        float val = 0.0f;
        // The compiler identifies this tight loop and generates SIMD instructions
        // (e.g., AVX2 on x86, NEON on ARM)
        for (int j = 0; j < n; j++) {
            val += w[i * n + j] * x[j];
        }
        out[i] = val;
    }
}
```
Compile with: `cc -O3 -ffast-math -fopenmp microgpt.c -o train`

Scenario: IoT sensor analyzing signals. Vectorization halves inference time, extending battery life.

## Accelerated Libraries: BLAS for Matrix Operations

BLAS (Basic Linear Algebra Subprograms) are optimized routines for operations like matmul.

Background: Libraries like Apple Accelerate or OpenBLAS provide hand-tuned assembly for hardware.

In MicroGPT-C, link BLAS for forward/backward passes. Example: cblas_sgemv for matrix-vector multiply.

Scenario: Training loop. Native loops: 280K tokens/sec. BLAS: 500K+ on multi-core, but threading overhead at small batches.

Math: Matmul m x n x p: O(mnp) flops. BLAS optimizes cache (block tiling: divide into sub-matrices), reducing misses.

Verification: Cache miss rate. Naive: 50% (random access). Tiled: 10%—2-5x faster for n=512.

Tradeoff: At MicroGPT-C scales (n=128), dispatch (function call) overhead ~10μs > compute 5μs. CPU loops win.

Example: 875K-param model inference: CPU SIMD 960K tok/s; BLAS 280K (thread contention).

Use BLAS for n>256; else, native.

## GPU Offloading: When Parallelism Pays Off

GPUs excel at parallel computations, like thousands of matmuls simultaneously.

Background: In transformers, attention and feed-forwards parallelize across dimensions. MicroGPT-C supports Metal (Apple GPUs) via shaders—small programs running on GPU cores.

Scenario: Inference on Mac (M-series). CPU: 16K tok/s; GPU: But at small n=128, GPU dispatch 50μs > compute, yielding only 18K tok/s—CPU wins.

Math Verification: Crossover: Compute time = dispatch when n~512 (matrix 512x512 ~50μs). Below: Overhead dominates.

Code: Offload lin_fwd (y = W x): Convert double to float (Metal limit), dispatch kernel.

Kernel Example (simplified):
```
kernel void matvec(device float *x, device float *W, device float *y, uint gid) {
    float sum = 0;
    for(uint i=0; i<nin; i++) sum += W[gid*nin + i] * x[i];
    y[gid] = sum;
}
```
Dispatch nout threads (one per output).

Tradeoff: Precision loss (float vs. double) negligible for noisy gradients. Unified memory (Apple): Zero-copy.

Verification Scenario: Train 460K model. GPU: Faster for large batches; edge devices rarely have GPUs.

## Quantization: Reducing Precision for Efficiency

Quantization compresses models by using fewer bits per parameter (e.g., INT8=8 bits vs. float32=32 bits).

Background: Parameters are floats; quantize to ints by scaling/rounding. Inference: 4x less memory, 2x faster (integer ops quicker).

In MicroGPT-C, optional INT8: Weights quantized post-training.

Math: Range [-r,r] to [-127,127]: Scale = 127/r; quant = round(value * scale).

Error: Mean quantization noise ~1/√(12*levels) ≈0.1 for 8-bit—tolerable for small models.

Scenario: Deploy on microcontroller (1MB RAM). Float: 2MB model too big; INT8: 0.5MB fits. Speed: Integer mul faster on embedded.

Verification: Accuracy drop <5% on retrieval tasks; retrain if needed.

Tradeoff: Training harder (gradients need dequant); use for inference.

## Memory Footprints and Management

Edge limits: <10MB RAM. Strategies: Paged KV cache (allocate chunks), share buffers.

Math: KV cache per layer: 2 * block_size * embd * sizeof(float) * heads. For 256x128x4x8: ~2MB/layer. Paging: Fixed pages (e.g., 64 tokens each), reuse.

Scenario: Long conversation bot. Flat cache overflows; paged grows dynamically.

Verification: Profile: Flat alloc fails at 1024 tokens; paged handles 4096 with same peak memory.

## Parameter Right-Sizing: The Biggest Win

Before reaching for SIMD, BLAS, or quantisation, the single most impactful edge optimisation is **using the right number of parameters**. The game portfolio experiment (Chapter 6) proved this conclusively: right-sizing 8 organelle models from a uniform 460K down to 3 corpus-matched tiers (30K/92K/160K) yielded:

- **65–93% smaller models** — fitting in tighter RAM budgets
- **3–10× faster training** — critical for on-device learning
- **4 of 8 games improved** — over-parameterisation actively hurts when the corpus is small

The rule of thumb: **params ≈ 5–20× corpus size**. Below 5× the model underfits; above 20× it memorises noise.

This is a first-order optimisation. Apply it before any of the techniques in this chapter — the savings compound with SIMD, quantisation, and memory management.

## End-to-End Research: Tradeoffs at Small Scales

Research shows: For <1M params, CPU SIMD > GPU/BLAS (50x in cases). Quantize for deployment.

Verification: Benchmark suite: Time vs. size. Crossover points guide choices.

Principle: Optimize for your hardware—profile always.