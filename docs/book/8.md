# Chapter 8: Attention Mechanisms and Scaling

## Introduction

Attention is the mechanism that lets a transformer focus on relevant parts of its input. This chapter examines standard multi-head attention (MHA)—which is implemented in `src/microgpt.c`—and efficient alternatives like grouped query attention (GQA) and sliding window attention (SWA), which are documented in `docs/foundation/ATTENTION_MECHANISMS.md` as planned extensions.

Scaling—increasing model size or context length—ties in directly, with tradeoffs for edge deployment. Smarter attention unlocks longer contexts and better performance, but at small scales, efficiency variants prevent waste.

## Core Attention: How Models Focus

Attention computes relationships between input elements, weighting them based on relevance.

Background for Beginners: In a sequence (e.g., words), each position generates a query (what I need), while others provide keys (what I offer) and values (content). Similarity between query and keys determines weights.

Formula Recap (from Chapter 2): Attention(Q, K, V) = softmax( (Q K^T) / sqrt(d_k) ) V

Where Q, K, V are derived from input via linear transformations, d_k is key dimension (e.g., 16), sqrt scales to stabilize.

In generative models, it's causal: Mask future positions (set scores to -infinity before softmax) so predictions use only past.

Scenario: Sentence "The animal didn't cross the street because it was too...". Attention on "tired" weights "animal" high, "street" low—contextual understanding.

Math Verification: Vectors Q=[1,0], K1=[1,0] (past), K2=[0,1]. Scores: [1/sqrt(2), 0/sqrt(2)]. Softmax: [0.622, 0.378]. Output blends V1 more.

Efficiency Issue: Quadratic time/memory O(n^2) for sequence n—problem for long contexts.

## Multi-Head Attention (MHA): Parallel Perspectives

MHA runs multiple attention "heads" in parallel, each with its own Q/K/V projections, then concatenates outputs.

Background: Heads learn different relations (e.g., one for syntax, one for semantics). Typical: 8 heads, each d_head = d_model / heads (e.g., 128/8=16).

Formula: Head_i = Attention(Q W_qi, K W_ki, V W_vi); Output = concat(heads) W_o

Params: 3 d^2 per head (Q/K/V) + d^2 for output—total 4 d^2 per layer.

Scenario: Name generation with history. One head attends to prefixes ("Mc" → Scottish), another to lengths.

Math: Quality gain: Single head misses nuances; multi: Lower perplexity (exp(loss)) by 10-20% on text.

Tradeoff: KV cache (stored K/V for inference) per layer: 2 n d_model (duplicated per head in naive).

Verification Example: Train on repeating patterns ("ABAB..."). Single head: Loss=0.5; MHA: 0.1—better captures multiples.

## Grouped Query Attention (GQA): Sharing for Efficiency

> **Note:** GQA is a planned extension documented in `docs/foundation/ATTENTION_MECHANISMS.md`. It is not yet implemented in `microgpt.c`.

GQA reduces redundancy by sharing K/V across groups of Q heads.

Background: In MHA, each head has unique K/V—wasteful if similar. GQA groups queries (e.g., 8 Q heads, 2 KV groups: 4 Q per KV).

Formula: For group g: Attention(Q_{4g:4g+4}, shared_K_g, shared_V_g); Concat as MHA.

Params/Memory: KV reduced by factor groups/heads (e.g., 1/4). Cache: Halves for 2 groups.

Scenario: Long story generation. MHA cache fills RAM at n=1024; GQA handles n=2048.

Math Verification: Memory = layers * 2 * n * d * heads (MHA) vs. * groups (GQA). For heads=8, groups=2: 75% savings, quality drop <1% (empirical on benchmarks).

Tradeoff: Slight generality loss, but negligible at small scales.

Example: Puzzle history (past states). GQA shares board evaluations, efficient for retrieval.

## Sliding Window Attention (SWA): Limiting Scope for Long Contexts

> **Note:** SWA is a planned extension. See `docs/foundation/ATTENTION_MECHANISMS.md` for discussion.

SWA restricts attention to a window of recent tokens, ignoring distant past.

Background: Most relevance is local (e.g., last 512 tokens). Global tokens (e.g., summary) can handle far-back if needed.

Formula: Mask scores outside window w (e.g., 256): Set to -inf for |i-j| > w.

Compute: O(n w) vs. O(n^2)—linear for fixed w.

Scenario: Chatbot with long history. Full attention: Slow at 10K tokens. SWA: Constant time, focuses on recent dialog.

Math: Speedup = n / w. For n=1024, w=256: 4x. Memory same, but cache trims old.

Verification: On Shakespeare (long text): Full loss=1.2; SWA w=512: 1.25 (minor drop), 2x faster.

Tradeoff: Loses global context; combine with GQA for balance.

## Multi-Query Attention (MQA) and Beyond

MQA is extreme GQA (1 KV group for all Q)—minimal memory, for very long contexts.

Background: Like all heads sharing one note set—efficient but less personalized.

Future: Multi-Layer Attention (MLA)—stacks for deeper relations.

Scenario: Sensor data stream (endless). MQA cache tiny, handles infinite context theoretically.

Math: KV factor=1/heads (e.g., 1/8 savings vs. MHA).

## Scaling Impacts: Embeddings, Layers, and Context

Scaling: Increase d (embed), l (layers), n (context).

Tradeoffs:
- d up: Richer representations, but params ∝ d^2 (quadratic cost).
- l up: Deeper reasoning, linear cost.
- n up: Longer memory, but KV cache ∝ n d l—OOM risk.

Scenario: Scale d=48→96: Parse errors drop 91% in games (more capacity for patterns).

Math Verification: Params total ~ l * 12 d^2 (approx). At d=96, l=4: ~460K. Inference time ∝ l d^2 + n^2 / heads.

End-to-End Experiment: Shakespeare demo. Base (d=32, n=32): Loss=1.5. Scaled (d=64, n=128): 1.0. GQA+SWA: Handles n=512 at same memory.

Verification: Sequence scaling test—generate 100 tokens: Time linear with SWA, quadratic without.