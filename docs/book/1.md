
**Preface: Why This Guide?**  
For hackers building edge AI without the bloat. Tired of GPU farms? Let's hack AI on a Pi. This guide equips you with the code, math, and challenges to prototype your own edge agents from scratch in pure C99.

This guide provides a technical walkthrough of MicroGPT-C: a zero-dependency C99 transformer framework for composable, sub-1M parameter AI at the edge. It assumes the reader has the repository alongside and basic C programming knowledge.

# Detailed Chapters Overview

The guide is structured as a progressive reference, starting with foundational concepts and building toward advanced applications and research. Each chapter includes code references, mathematical verification, and pointers to specific source files and project documentation.

### Part I: Foundations – Understanding the Core Framework

1. **[Chapter 1: The Case for Small AI](1.md)**  
   *Teaser: Discover why massive LLMs are unsustainable for edge devices and calculate your own device's parameter limits.*  
   Why massive LLMs are unsustainable for edge devices and how MicroGPT-C provides a zero-dependency C99 alternative. Covers the "generalist monolith" problem, the stem cell analogy, and the value of specialized, composable models.  
   Key Topics: AI accessibility challenges; memory/compute math for edge devices; end-to-end research preview.

2. **[Chapter 2: Core Architecture of MicroGPT-C](2.md)**  
   *Teaser: Dive into the forward pass with pseudocode for attention: `Attention(Q, K, V) = softmax(Q K^T / sqrt(d_k)) V`. Swap ReLU for GELU and measure the loss delta.*  
   The GPT-2-style transformer implementation in `src/microgpt.h` and `src/microgpt.c`. Covers tokenization (character-level and word-level), multi-head attention with causal masking, ReLU-activated feed-forward layers, RMSNorm with pre-normalization, and the Adam optimizer. References `docs/foundation/ATTENTION_MECHANISMS.md` for planned attention variants.  
   Key Topics: Parameter-efficient design; forward/backward passes; pre-norm block order; sub-1M parameter scaling.

3. **[Chapter 3: Training and Inference Fundamentals](3.md)**  
   *Teaser: Implement the Adam update formula from scratch and modify the LR schedule to plot your own loss curves.*  
   Training loops, cross-entropy loss, learning rate scheduling (warmup + cosine decay), KV caching (including paged variants via `MICROGPT_PAGED_KV`), and reproducibility via seeded PRNG. References `docs/foundation/TRAINING_STRATEGIES.md`.  
   Key Topics: On-device training; handling catastrophic forgetting; checkpointing and resume.

### Part II: Building Organelles – Specialization and Composition

4. **[Chapter 4: The Organelle Concept – From Stem Cells to Specialists](4.md)**  
   *Teaser: Watch a generic stem cell differentiate. Generate your own logic puzzles with a Python script and train a specialized model.*  
   The organelle API (`src/microgpt_organelle.h`, `src/microgpt_organelle.c`) and the differentiation process. Covers corpus generation, retrieval-based intelligence, ensemble voting, valid-move filtering, capacity scaling (64K to 460K params), and the 3-tier parameter right-sizing strategy (30K–160K). References `docs/organelles/ORGANELLE_VISION.md`.  
   Key Topics: Organelle training and inference; ensemble confidence; fallback mechanisms; parameter-corpus matching.

5. **[Chapter 5: Pipeline Coordination – The Kanban Architecture](5.md)**  
   *Teaser: Build an Observe-Plan-Act (OPA) pipeline and see how Kanban state machines (`OpaKanban`) prevent models from making invalid moves.*  
   The Organelle Pipeline Architecture (OPA) from `docs/organelles/ORGANELLE_PIPELINE.md`, including Kanban state management (`OpaKanban`), cycle detection (`OpaCycleDetector`), and the Planner-Worker-Judge decomposition. Uses game demos as case studies.  
   Key Topics: "Coordination rescues weakness"; invalid move filtering; oscillation breaking and replanning.

6. **[Chapter 6: Logic Games as Research Laboratories](6.md)**  
   *Teaser: Climb the game progression ladder from Tic-Tac-Toe to Red Donkey. See how a 30K parameter model can outsmart a 160K parameter one.*  
   Why games are ideal for testing OPA. Analyses eleven game demos from `experiments/organelles/`: 8-Puzzle, Tic-Tac-Toe, Connect-4, Lights Out, Mastermind, Klotski, Sudoku, Othello, Hex, Pentago, and Red Donkey. Covers decomposition patterns, win rate metrics, the game portfolio progression, and the parameter right-sizing experiment. References `docs/organelles/ORGANELLE_GAMES.md`.  
   Key Topics: Controlled testing environments; 11-game validation of OPA; transferring game insights to real domains.

### Part III: Optimizations and Advanced Techniques

7. **[Chapter 7: Optimization Strategies for Edge Deployment](7.md)**  
   *Teaser: Squeeze every flop out of your CPU. Compare SIMD vectorization vs. scalar loops and benchmark inference speed.*  
   CPU SIMD vectorization, BLAS integration (`MICROGPT_BLAS`), GPU offloading via Metal (`src/microgpt_metal.h`, `src/microgpt_metal.m`), INT8 quantization (`QUANTIZATION_INT8`), and memory footprint management. References `docs/foundation/OPTIMISATION_STRATEGIES.md`.  
   Key Topics: Small-scale tradeoffs (CPU beats GPU below 512 embed dim); tiled matmul; dispatch overhead.

8. **[Chapter 8: Attention Mechanisms and Scaling](8.md)**  
   *Teaser: Dive into the math of Grouped-Query Attention (GQA) and visualize the matrix differences between MHA, MQA, and GQA.*  
   Implemented MHA and its scaling properties. Planned extensions: GQA and SWA (documented in `docs/foundation/ATTENTION_MECHANISMS.md`). Covers memory vs. quality tradeoffs and long-context handling.  
   Key Topics: KV cache efficiency; scaling embed/layers/context; planned MLA integration.

9. **[Chapter 9: Tooling and Workflow – From Research to Production](9.md)**  
   *Teaser: From hypothesis to deployment. Use CLI tools to profile models and achieve reproducible training runs.*  
   Benchmarking, testing strategies (`tests/`), corpus management, multi-threaded training (`src/microgpt_thread.h`), and the planned CLI. Covers reproduction via seeding and automated workflows.  
   Key Topics: Reproducibility; profiling; the planned CLI tool (see ROADMAP.md).

### Part IV: Real-World Applications and Future Directions

10. **[Chapter 10: Code Generation and Structured Outputs](10.md)**  
    *Teaser: Parse a flat-string protocol tree and achieve 83% exact match on function composition plans with a 1.2M parameter model.*  
    Autonomous code synthesis using `c_codegen`, `c_wiringgen`, and `c_compose` experiments. The c_compose pipeline (Planner→Judge, 1.2M params with LR scheduling) achieves **83% exact match**. Covers flat-string protocols, paraphrase blindness mitigation, and confidence gating.  
    Key Topics: Composition accuracy; pipeline-based code generation; LR scheduling; structured output validation.

11. **[Chapter 11: Edge AI Applications – Sensors, IoT, and Beyond](11.md)**  
    *Teaser: Deploy an IoT pipeline on an ESP32 microcontroller mapping sensor nodes to actionable intelligence.*  
    Applying OPA to sensors, IoT, and robotics. Covers on-device adaptation and conceptual federated differentiation patterns. Discusses deploying on ESP32 and similar microcontrollers.  
    Key Topics: Anomaly detection; real-time inference; privacy via local training.

12. **[Chapter 12: Ethical Considerations and Safeguards](12.md)**  
    *Teaser: Quantify bias with math and use judge patterns to securely gate your output.*  
    Risks (overconfidence, data bias, privacy leaks) and mitigations (curated corpora, judge patterns, confidence thresholds, drift detection). References the safety approach from `README.md`.  
    Key Topics: Bias auditing; validation loops; transparency via attention inspection.

13. **[Chapter 13: Future Research and Extensions](13.md)**  
    *Teaser: Approximate LoRA ranks for future edge training and outline the timeline for scaling to 1M+ parameter DAG pipelines.*  
    Research proposals aligned with `ROADMAP.md`: organelle marketplace, self-monitoring, hardware targets (RISC-V, FPGA), hybrid approaches (search + transformers), and open questions.  
    Key Topics: Scaling to 1M+ params; DAG pipelines; community contributions.

### Appendices
- **[Appendix A: Glossary and References](A.md)** – Key terms (e.g., kanban, OPA) with integrated mathematical equations, citations to foundational papers, and reference links to full code listings, benchmarks, and datasets.
