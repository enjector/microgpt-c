# Chapter 3: Training and Inference Fundamentals

## Introduction

With MicroGPT-C's architecture in place (Chapter 2), we now turn to the dynamic aspects: training and inference. Training is where the model learns from data, adjusting its parameters to make better predictions. Inference is where the trained model generates outputs based on new inputs.

This chapter covers training loops, learning rate scheduling, optimization with Adam, KV caching for efficient inference, and techniques for reproducibility. By the end, you'll understand how to train a model from scratch and deploy it for practical use on edge devices.

## The Training Loop: Learning from Data

Training in MicroGPT-C follows a loop: feed data, compute errors, adjust parameters, repeat. The goal is to minimize "loss"—a numerical measure of how wrong the model's predictions are.

### Loss Function: Measuring Errors

The primary loss in generative models like MicroGPT-C is cross-entropy, which quantifies the difference between predicted probabilities and actual targets.

Background for Beginners: Suppose the model predicts the next token in a sequence. It outputs probabilities for each possible token (e.g., 0.7 for 'a', 0.2 for 'b', 0.1 for others). If the true next token is 'a', a low loss rewards high probability for 'a'; high loss penalizes low confidence.

Formula: Loss = -sum over tokens [target_prob * log(predicted_prob)]. For one token, if target is 1 (certain) and predicted is p, loss = -log(p). If p=0.9, loss≈0.105; if p=0.1, loss≈2.3—much higher.

Verification Example: Train on "hello". Tokenized as [h,e,l,l,o]. Model predicts after "hell" should be 'o' (prob 0.01 initially, loss high). After training, prob=0.99, loss low (0.01).

In code, MicroGPT-C computes this in the forward-backward pass, averaging over batches (groups of examples) for stability.

Scenario: Building a password strength checker. Train on strong/weak examples. High loss on weak passwords encourages the model to flag patterns like "1234".

### Batches and Epochs

Data is processed in batches (e.g., 32 examples) to balance speed and accuracy. An epoch is one full pass through the dataset.

Math Verification: Gradient descent (parameter update) is more stable with batches. Variance in single-example gradients is high; averaging reduces noise. If dataset has 1,000 examples, batch size 100 means 10 updates per epoch.

In MicroGPT-C, multi-threading splits batches across CPU cores, speeding training on laptops.

## Optimization: Adjusting Parameters Efficiently

Optimization uses gradients (derivatives showing how to reduce loss) to update parameters.

### Adam Optimizer

MicroGPT-C uses Adam, which adapts learning rates per parameter.

Background: Basic gradient descent: param -= learning_rate * gradient. Adam adds momentum (smooths updates) and adaptive scaling (larger steps for stable parameters).

Formulas:
- First moment: m = β1 * m + (1 - β1) * g (β1=0.85)
- Second moment: v = β2 * v + (1 - β2) * g² (β2=0.99)
- Update: param -= lr * m / (sqrt(v) + ε) (ε=1e-8 prevents division by zero)

This prevents overshooting in noisy gradients, common in small datasets.

Example Scenario: Training a sentiment analyzer on reviews ("good" vs. "bad"). Initial gradients swing wildly; Adam damps them, converging faster than basic methods.

Verification with Toy Math: Gradient g=1.0, initial m=v=0. After one step: m=0.15, v=0.01, update ≈ lr * 0.15 / 0.1 = 1.5 lr. Next g=0.5: m≈0.1775, v≈0.01099, update smaller—adapts.

### Learning Rate Scheduling

Raw learning rates can cause divergence (exploding loss). MicroGPT-C uses warmup (start low, ramp up) and cosine decay (smooth decrease).

Warmup: lr = base_lr * min(1, step / warmup_steps)

Cosine: lr = base_lr * 0.5 * (1 + cos(π * (step - warmup) / (total - warmup)))

Why? Early steps need caution; later, fine-tuning.

Scenario: Music note predictor. Without scheduling, model oscillates; with it, steady improvement.

Math Check: At step=0, lr=0; midpoint, lr=base; end, lr=0—prevents overfitting.

### Case Study: Stabilising 1.2M Parameters (c_compose v3)

The c_compose experiment (Chapter 10) demonstrates why LR scheduling tuning matters as models scale.

| Setting | lr | warmup | Result |
|---------|------|--------|--------|
| v2 (default) | 0.001 | 100 | **Diverged** — loss exploded, 20% parse rate, 0% exact match |
| v3 (tuned) | 0.0005 | 2500 | **Stable** — 98% parse, 83% exact match, 96% judge PASS |

Two changes made the difference:
- **Halved peak lr** (0.001→0.0005): Larger models have more parameters competing for gradient signal; smaller steps prevent overshooting.
- **25× longer warmup** (100→2500, 5% of 50K steps): Gives Adam's moment estimates time to stabilise before full-strength updates.

Rule of thumb: lr ∝ 1/√params. At 460K params, lr=0.001 works. At 1.2M params, lr=0.0005 is needed. See `docs/foundation/TRAINING_STRATEGIES.md` for full empirical evidence and recommended hyperparameters by model scale.

## Inference: Generating Outputs

Inference reuses the forward pass but samples from probabilities instead of computing loss.

### Sampling Techniques

- Greedy: Pick max probability (deterministic, repetitive).
- Temperature: Scale logits before softmax. Temp=0: greedy; Temp=1: original; Temp>1: more random.

Formula: logits / temp → softmax.

Verification: Logits [2,1,0] at temp=1: probs [0.665,0.245,0.09]. At temp=0.5: sharper [0.88,0.106,0.014]—less random.

Top-k: Sample from top k probs. Nucleus (top-p): From cumulative probs until sum>p.

Scenario: Story generator. High temp for creativity ("The dragon flew to Mars"); low for consistency.

### KV Caching for Efficiency

In sequences, recompute past attention each time? No—cache keys/values, append new.

Background: For position t, attention uses keys/values up to t-1.

Math: Without cache, time O(t²) per generation; with, O(t) total.

Paged KV: For long sequences, allocate in pages to avoid fragmentation.

Example: Chatbot. Cache conversation history; add user input, generate response quickly.

## Reproducibility and Overfitting

Seed random number generators for same results across runs.

Overfitting: Model memorizes training data, fails on new. Detect: Train loss low, test loss high.

Scenario: Train on 10 names, generates perfectly—but on unseen, garbage. Solution: More data, regularization (e.g., dropout: randomly ignore nodes).

Math: Compute perplexity = exp(loss). Low=good prediction. Overfit: Train perplexity=1 (perfect), test=10 (poor).

## Handling Catastrophic Forgetting

Incremental training erases old knowledge. Mitigate with replay buffers (mix old/new data).

Verification: Train on set A (loss=0.1), then B (A loss rises to 1.0). With replay, A loss stays low.