# Attention Mechanisms and Scaling

## Introduction

Attention is the mechanism that lets a transformer focus on relevant parts of its input. This chapter examines standard multi-head attention (MHA)—which is implemented in the core engine—and efficient alternatives like grouped query attention (GQA) and sliding window attention (SWA) which are planned extensions.

Scaling—increasing model size or context length—ties in directly, with tradeoffs for edge deployment. Smarter attention unlocks longer contexts and better performance, but at small scales, efficiency variants prevent waste.

## Core Attention: How Models Focus

Attention computes relationships between input elements, weighting them based on relevance.

Background for Beginners: In a sequence (e.g., words), each position generates a query (what I need), while others provide keys (what I offer) and values (content). Similarity between query and keys determines weights.

Formula Recap (from Chapter 2): Attention(Q, K, V) = softmax( (Q K^T) / sqrt(d_k) ) V

Where Q, K, V are derived from input via linear transformations, d_k is key dimension (e.g., 16), sqrt scales to stabilize.

In generative models, it's causal: Mask future positions (set scores to -infinity before softmax) so predictions use only past.

Scenario: Sentence "The animal didn't cross the street because it was too...". Attention on "tired" weights "animal" high, "street" low—contextual understanding.

Math Verification: Vectors Q=[1,0], K1=[1,0] (past), K2=[0,1]. Scores: [1/sqrt(2), 0/sqrt(2)]. Softmax: [0.622, 0.378]. Output blends V1 more.

Efficiency Issue: Quadratic time/memory O(n^2) for sequence n—problem for long contexts.

## Multi-Head Attention (MHA): Parallel Perspectives

MHA runs multiple attention "heads" in parallel, each with its own Q/K/V projections, then concatenates outputs.

Background: Heads learn different relations (e.g., one for syntax, one for semantics). Typical: 8 heads, each d_head = d_model / heads (e.g., 128/8=16).

Formula: Head_i = Attention(Q W_qi, K W_ki, V W_vi); Output = concat(heads) W_o

Params: 3 d^2 per head (Q/K/V) + d^2 for output—total 4 d^2 per layer.

Scenario: Name generation with history. One head attends to prefixes ("Mc" → Scottish), another to lengths.

Math: Quality gain: Single head misses nuances; multi: Lower perplexity (exp(loss)) by 10-20% on text.

Tradeoff: KV cache (stored K/V for inference) per layer: 2 n d_model (duplicated per head in naive).

Verification Example: Train on repeating patterns ("ABAB..."). Single head: Loss=0.5; MHA: 0.1—better captures multiples.

## Grouped Query Attention (GQA): Sharing for Efficiency

> **Speculative / Future Work:** Grouped Query Attention (GQA) is a planned architectural extension to reduce KV cache memory. While the mathematical foundation is solid, it is not yet implemented in the core `microgpt.c` engine.

GQA reduces memory redundancy by sharing keys and values across groups of query heads.

**Visualization: MHA vs. GQA vs. MQA**

![MHA vs GQA](gqa_bw.png)

**Mathematical Formulation:**
In standard MHA with $H$ heads, each head $h \in \{1, \dots, H\}$ has its own projection matrices $W_q^{(h)}, W_k^{(h)}, W_v^{(h)}$. 
In GQA, we define $G$ groups, where $1 \le G \le H$. The heads are divided such that each group contains $H / G$ queries sharing a single key and value pair.
For head $h$ in group $g = \lfloor \frac{h \cdot G}{H} \rfloor$:
\begin{equation}
\text{Attention}^{(h)}(X) = \text{softmax}\left(\frac{(X W_q^{(h)})(X W_k^{(g)})^T}{\sqrt{d_k}}\right) (X W_v^{(g)})
\end{equation}

Params/Memory: KV cache is reduced by a factor of $G/H$. For example, with $H=8$ queries and $G=2$ KV groups, cache size shrinks by 75%.

Scenario: Long story generation on a 1MB RAM edge device. MHA's KV cache exhausts RAM at $n=1024$. GQA ($G=2$) handles $n=2048$ with identical memory pressure.

Math Verification: Memory = $\text{layers} \times 2 \times n \times d \times \text{heads}$ (MHA) vs. $\text{groups}$ (GQA). For an organelle ($d=64$, $l=3$, $H=4$), $n=2048$: MHA KV cache is $\sim 6$ MB. GQA with $G=1$ (MQA) reduces this to $\sim 1.5$ MB, enabling inference on deeply constrained microcontrollers.

Tradeoff: Slight generality loss, but negligible at small scales.

Example: Puzzle history (past states). GQA shares board evaluations across specialized query heads (e.g., threat detection and goal-seeking), highly efficient for retrieval.

## Sliding Window Attention (SWA): Limiting Scope for Long Contexts

> **Speculative / Future Work:** Sliding Window Attention (SWA) is a planned conceptual extension to restrict attention context and prevent memory exhaustion for long sequences. It is currently unimplemented.

SWA restricts attention to a window of recent tokens, ignoring distant past.

Background: Most relevance is local (e.g., last 512 tokens). Global tokens (e.g., summary) can handle far-back if needed.

Formula: Mask scores outside window w (e.g., 256): Set to -inf for |i-j| > w.

Compute: O(n w) vs. O(n^2)—linear for fixed w.

Scenario: Chatbot with long history. Full attention: Slow at 10K tokens. SWA: Constant time, focuses on recent dialog.

Math: Speedup = n / w. For n=1024, w=256: 4x. Memory same, but cache trims old.

Verification: On Shakespeare (long text): Full loss=1.2; SWA w=512: 1.25 (minor drop), 2x faster.

Tradeoff: Loses global context; combine with GQA for balance.

## Multi-Query Attention (MQA) and Beyond

MQA is extreme GQA (1 KV group for all Q)—minimal memory, for very long contexts.

Background: Like all heads sharing one note set—efficient but less personalized.

Future: Multi-Layer Attention (MLA)—stacks for deeper relations.

Scenario: Sensor data stream (endless). MQA cache tiny, handles infinite context theoretically.

Math: KV factor=1/heads (e.g., 1/8 savings vs. MHA).

## Scaling Impacts: Embeddings, Layers, and Context

Scaling: Increase d (embed), l (layers), n (context).

Tradeoffs:

- d up: Richer representations, but params ∝ d^2 (quadratic cost).
- l up: Deeper reasoning, linear cost.
- n up: Longer memory, but KV cache ∝ n d l—OOM risk.

Scenario: Scale d=48→96: Parse errors drop 91% in games (more capacity for patterns).

Math Verification: Params total ~ l * 12 d^2 (approx). At d=96, l=4: ~460K. Inference time ∝ l d^2 + n^2 / heads.

End-to-End Experiment: Shakespeare demo. Base (d=32, n=32): Loss=1.5. Scaled (d=64, n=128): 1.0. GQA+SWA: Handles n=512 at same memory.

Verification: Sequence scaling test—generate 100 tokens: Time linear with SWA, quadratic without.