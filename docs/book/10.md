# Tooling and Workflow – From Research to Production

## Introduction

This chapter covers the tools and workflows that bridge research experimentation with production deployment. Tooling includes command-line interfaces, testing suites, and benchmarks (see `tests/` and the benchmark scaffolding in `src/microgpt.c`). Workflows define the step-by-step processes for iterating from idea to deployed system.

Good tooling reduces friction, allowing focus on innovation rather than boilerplate.

## Command-Line Interface (CLI): Simplifying Model Management

> **Note:** The CLI described below is a planned feature on the project roadmap. Current usage involves compiling individual demo programs directly.

A CLI provides a unified interface for running commands like training or inference without writing a custom main program each time.

In MicroGPT-C, the CLI (planned as a single binary) supports commands like create (new model), train (from corpus/checkpoint), infer (generate), and pipeline (run multi-organelle flows).

Scenario: Researching name generation. CLI: Create config, train on corpus, infer samples. Production: Script CLI calls for batch processing.

Math Verification: Time savings. Manual script: 30min setup + run. CLI: 1min command—30x faster for iterations. Error rate: Manual typos 10%; CLI validation 1%.

Code Example (usage):
```bash
# 1. Initialize a blank "stem cell" model
microgpt create --n_embd 96 --n_layer 4 --name my_sensor_model

# 2. Differentiate it by training on a custom corpus
microgpt train --corpus sensor_data.txt --checkpoint my_sensor_model.ckpt --steps 5000 --lr 0.001

# 3. Test inference manually
microgpt infer --prompt "temp=45|humidity=80" --max_len 20

# 4. Run it inside an autonomous pipeline
microgpt pipeline --config sensor_opa.ini
```

Workflow Tip: Use INI configs for reusability (e.g., `[model] n_embd=96`).

Verification Scenario: Train two variants. CLI resumes from checkpoints; compare losses—ensures reproducibility.

## Benchmarking: Measuring Performance

Benchmarks time operations (e.g., inference speed) to identify bottlenecks.

Background: Metrics like tokens/second (tok/s) or milliseconds per forward pass guide optimizations.

In MicroGPT-C, benchmarks cover forward/inference, training steps, tokenization, and scaling (vary embed/layers).

Scenario: Edge device deployment. Benchmark inference: Base 16K tok/s. After vectorization (Chapter 7): 50K. Too slow? Quantize.

Math: Throughput = tokens / time. For n=256 sequence, time=10ms: 25.6K tok/s. Scale: Double embed, time ~4x (quadratic in parts)—predicts limits.

Code Snippet (simple benchmark):
```
clock_t start = clock();
for(int i=0; i<100; i++) forward_inference(model, token, pos, ...);
double ms = (clock() - start) / (double)CLOCKS_PER_SEC * 1000.0;
printf("Avg time: %.2f ms\n", ms/100);
```

Verification: Run on CPU vs. optimized—quantify gains (e.g., 2x from tiling matmul: divide large matrices into cache-friendly blocks).

Workflow: Profile regularly; aim <50ms inference for real-time (e.g., voice assistant).

## Testing: Ensuring Reliability

Tests verify code and models work as expected, from unit (single function) to integration (full pipeline).

Background: Unit tests check isolated parts (e.g., attention output); integration tests whole flows.

In MicroGPT-C, suites test tokenization, forward/backward, KV cache, and pipeline components (e.g., cycle detection).

Scenario: Pipeline update breaks cycle breaker. Test: Simulate loop; assert detection. Prevents regressions.

Math Verification: Coverage = tested lines / total. Aim 80%+. Error detection: False positives <5% (e.g., for cycles: Window=4, short cycles detect 95%).

Code Example (pseudocode unit test):
```
void test_softmax() {
    float in[3] = {1,2,0};
    softmax(in, 3);
    assert(fabs(in[0]-0.245)<1e-3);  // Verify sums to 1, matches expected
}
```

Integration: Run puzzle pipeline on known boards; assert solve rate >85%.

Workflow: Test-driven development—write tests first, code to pass. For production: Automate on commits.

## Corpus Management: Data as the Foundation

Corpora drive differentiation (Chapter 4); good management ensures quality.

Background: Generate via scripts (e.g., simulate games), balance classes, shuffle to avoid order bias.

In MicroGPT-C, loaders handle multi-line files; shuffle docs for randomness.

Scenario: Adversarial corpus (e.g., puzzles with traps). Generate 10K via BFS (breadth-first search: explore states level-by-level). Verify balance: Count easy/hard—50/50.

Math: Diversity = unique states / total. Low=overfit. Shuffle entropy: Uniform distribution prevents sequential learning.

Code Tip: Python generator (but in C: loop simulations, write to file).

Verification: Train on unshuffled: Loss low but generalizes poorly (test loss high). Shuffled: Balanced.

Workflow: Version corpora; track provenance (source, size) for reproducibility.

## Multi-Threaded Training: Parallel Power

Use CPU cores to speed training by splitting batches.

Background: Threads run concurrently; assign batch slices per thread.

In MicroGPT-C, threads process docs independently, accumulate gradients.

Scenario: 100-doc batch, 4 cores: Each 25 docs—4x faster.

Math: Speedup = cores / (1 + overhead fraction). Overhead~10% (sync): 3.6x for 4 cores.

Code: Use pthread (or Windows equiv): Create threads, join, average grads.

Verification: Time single-thread 10s; multi 3s—matches math.

Tradeoff: Small batches—diminishing returns. Cap threads at batch_size.

Workflow: Default to cpu_count; monitor for contention.

## End-to-End Workflow: Research to Production

The true power of MicroGPT-C is the speed at which you can move from a hypothesis to a deployed edge pipeline.

**The Edge AI Research Loop:**

![Edge AI Research Loop](edge_loop_bw.png)

Scenario: Anomaly detector. Research: Test on sim data (games-like). Production: Embed in sensor script.

Verification: Track metrics—loss, speed, accuracy—across stages. Iterative: If test fails, refine corpus.

Principle: Automation (CLI/scripts) enables rapid cycles.